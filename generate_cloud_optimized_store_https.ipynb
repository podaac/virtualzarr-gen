{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180973dc-a4fc-43aa-bbc5-d56ce3e9edba",
   "metadata": {},
   "source": [
    "# VirtualiZarr Useful Recipes with NASA Earthdata\n",
    "\n",
    "#### *Author: Dean Henze, PO.DAAC*\n",
    "\n",
    "*Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e719d0-17d1-45e9-9933-cb49a7cef7a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook goes through several functionalities of the VirtualiZarr package to create virtual reference files, specifically using it with NASA Earthdata and utilizing the `earthaccess` package. It is meant to be a quick-start reference that introduces some key capabilities / characteristics of the package once a user has a high-level understanding of virtual data sets and the cloud-computing challenges they address (see references in the *Prerequisite knowledge* section below). In short, VirtualiZarr is a Python package to create \"reference files\", which can be thought of as road maps for the computer to efficiently navigate through large arrays in a single data file, or across many files. Once a reference file for a data set is created, utilizing it to open the data can speed up several processes including lazy loading, accessing subsets, and in some cases performing computations. Importantly, one can create a combined reference for all the files in a dataset and use it to lazy load / access the entire record at once.\n",
    "\n",
    "The functionalities of VirtualiZarr (with earthaccess) covered in this notebook are:\n",
    "\n",
    "1. **Getting Data File endpoints in Earthdata Cloud** which are needed for virtualizarr to create reference files.\n",
    "2. **Generating reference files for 1 day, 1 year, and the entire record of a ~750 GB data set**. The data set used is the Level 4 global gridded 6-hourly wind product from the Cross-Calibrated Multi-Platform project (https://doi.org/10.5067/CCMP-6HW10M-L4V31), available on PO.DAAC. This section also covers speeding up the reference creation using parallel computing. Reference files are saved in both JSON and PARQUET formats. The latter is an important format as it reduces the reference file size by ~30x in our tests. *Saving in ice chunk formats will be tested / covered in the coming months.*\n",
    "3. **Combining reference files (in progress)**. The ability to combine reference files together is valuable, for example to upate reference files for forward-streaming datasets when new data are available, without re-creating the entire record from scratch. However, with the current workflows and version of VirtualiZarr, this is not possible due to our use of a specific kwarg when creating the reference files. The workflow is still included here (with errors) because it is anticipated that this will be fixed in upcoming versions. Alternately, the use of ice chunk will also likely solve this issue (ice chunk functionality to be tested soon). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce1593-e158-44d4-aab8-9f91027a19ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Requirements, prerequisite knowledge, learning outcomes\n",
    "\n",
    "#### Requirements to run this notebook\n",
    "\n",
    "* Earthdata login account: An Earthdata Login account is required to access data from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account.\n",
    "\n",
    "* Compute environment: This notebook is meant to be run in the cloud (AWS instance running in us-west-2). We used an `m6i.4xlarge` EC2 instance (16 CPU's, 64 GiB memory) for the parallel computing sections. At minimum we recommend a VM with 10 CPU's to make the parallel computations in Section 2.2.1 faster.\n",
    "\n",
    "* Optional Coiled account: To run the section on distributed clusters, Create a coiled account (free to sign up), and connect it to an AWS account. For more information on Coiled, setting up an account, and connecting it to an AWS account, see their website [https://www.coiled.io](https://www.coiled.io). \n",
    "\n",
    "#### Prerequisite knowledge\n",
    "\n",
    "* This notebook covers virtualizarr functionality but does not present the high-level ideas behind it. For an understanding of reference files and how they are meant to enhance in-cloud access to file formats that are not cloud optimized (such netCDF, HDF), please see e.g. this [kerchunk page](https://fsspec.github.io/kerchunk/), or [this page on virtualizarr](https://virtualizarr.readthedocs.io/en/latest/).\n",
    "\n",
    "* Familiarity with the `earthaccess` and `Xarray` packages. Familiarity with directly accessing NASA Earthdata in the cloud. \n",
    "\n",
    "* The Cookbook notebook on [Dask basics](https://podaac.github.io/tutorials/notebooks/Advanced_cloud/basic_dask.html) is handy for those new to parallel computating.\n",
    "\n",
    "#### Learning Outcomes\n",
    "\n",
    "This notebook serves both as a pedagogical resource for learning several key workflows as well as a quick reference guide. Readers will gain the understanding to combine the virtualizarr and earthaccess packages to create virtual dataset reference files for NASA Earthdata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f65dd1-39f6-480a-aa63-adbbd9863e8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import Packages\n",
    "#### ***Note Zarr Version***\n",
    "***Zarr version 2 is needed for the current implementation of this notebook, due to (as of February 2025) Zarr version 3 not accepting `FSMap` objects.***\n",
    "\n",
    "We ran this notebook in a Python 3.12 environment. The minimal working environment we used to run this notebook was:\n",
    "```\n",
    "zarr==2.18.4\n",
    "fastparquet==2024.5.0\n",
    "xarray==2025.1.2\n",
    "earthaccess==0.11.0\n",
    "fsspec==2024.10.0\n",
    "dask==2024.5.2 (\"dask[complete]\"==2024.5.2 if using pip)\n",
    "h5netcdf==1.3.0\n",
    "matplotlib==3.9.2\n",
    "jupyterlab\n",
    "jupyter-server-proxy\n",
    "virtualizarr==1.3.0\n",
    "kerchunk==0.2.7\n",
    "```\n",
    "And optionally:\n",
    "```\n",
    "coiled==1.58.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0b1c0c-c8f9-412c-8038-4b674de896c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Built-in packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Filesystem management \n",
    "import fsspec\n",
    "import earthaccess\n",
    "\n",
    "# Data handling\n",
    "import xarray as xr\n",
    "from virtualizarr import open_virtual_dataset\n",
    "\n",
    "# Parallel computing \n",
    "import multiprocessing\n",
    "from dask import delayed\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, print\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import urllib3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d821e530-0ff7-4ac5-add2-6471ec9e8a58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "collection = \"ASCATB-L2-Coastal\"\n",
    "loadable_coord_vars = [\"lat\",\"lon\"]\n",
    "start_date = \"1-1-2022\" # like 1-1-2022\n",
    "end_date = \"1-3-2022\"# None # like 1-1-2025\n",
    "bucket = \"podaac-thredds-sit\"\n",
    "debug = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019475fd-5b64-42e8-b49a-18e743905cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: ASCATB-L2-Coastal\n",
      "Vars: ['lat', 'lon']\n",
      "Bucket: podaac-thredds-sit\n",
      "start_date: 1-1-2022\n",
      "end_date: 1-3-2022\n"
     ]
    }
   ],
   "source": [
    "print(\"Collection: {}\".format(collection))\n",
    "print(\"Vars: {}\".format(loadable_coord_vars))\n",
    "print(\"Bucket: {}\".format(bucket))\n",
    "print(\"start_date: {}\".format(start_date))\n",
    "print(\"end_date: {}\".format(end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e58626-85f4-4fed-b5af-04736ca6f83d",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c5a123-6025-4a85-a7b0-4b9b747a9a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x30d21a810>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.set_options( # display options for xarray objects\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_coords=True,\n",
    "    display_expand_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901e7c4-66ca-4dfb-bd8f-aaf2f0291764",
   "metadata": {},
   "source": [
    "## 1. Get Data File https endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20dbc070-d5f7-407e-b92e-4fda1b8a82ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x1071d8dd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Earthdata creds\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7519aab1-b2aa-40fa-862a-62ed69439ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS creds. Note that if you spend more than 1 hour in the notebook, you may have to re-run this line!!!\n",
    "# fs = earthaccess.get_s3_filesystem(daac=\"PODAAC\")\n",
    "fs = earthaccess.get_fsspec_https_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11d5e609-988d-4168-89a0-a94afb8f8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Get the urllib3 logger\n",
    "    log = logging.getLogger('urllib3')\n",
    "    \n",
    "    # Set the logging level to DEBUG\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Create a stream handler to output logs to the console\n",
    "    # ch = logging.StreamHandler()\n",
    "    # ch.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Add the handler to the logger\n",
    "    # log.addHandler(ch)\n",
    "    from http.client import HTTPConnection\n",
    "    \n",
    "    # Set the debug level for HTTPConnection\n",
    "    HTTPConnection.debuglevel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "487b4dd6-39c6-4d7e-8051-eddcd22e2a4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Locate CCMP file information / metadata:\n",
    "if start_date != None or end_date != None:\n",
    "    granule_info = earthaccess.search_data(\n",
    "        short_name=collection,\n",
    "        temporal=(start_date, end_date)\n",
    "    )\n",
    "else:\n",
    "    granule_info = earthaccess.search_data(\n",
    "        short_name=collection,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc311a3-df1a-4565-a314-658bff759203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ASCATB-L2-Coastal/ascat_20211231_224800_metopb_48194_eps_o_coa_3202_ovw.l2.nc',\n",
       " 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ASCATB-L2-Coastal/ascat_20220101_003000_metopb_48195_eps_o_coa_3202_ovw.l2.nc',\n",
       " 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/ASCATB-L2-Coastal/ascat_20220101_021200_metopb_48196_eps_o_coa_3202_ovw.l2.nc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get S3 endpoints for all files:\n",
    "data_s3links = [g.data_links(access=\"https\")[0] for g in granule_info]\n",
    "data_s3links[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20756ad2-4fdf-4f7a-8582-aa7d59ea35e5",
   "metadata": {},
   "source": [
    "## 2. Generate reference files for 1 day, 1 year, and entire record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146d00a-cf61-4ed4-96e2-6db0a19bb466",
   "metadata": {},
   "source": [
    "### 2.1 First day\n",
    "The virtualizarr function to generate reference information is compact. We use it on one file for demonstration.\n",
    "\n",
    "***Important***\n",
    "\n",
    "The kwarg `loadable_variables` is not mandatory to create a viable reference file, but will become important for rapid lazy loading when working with large combined reference files. Assign to this at minimum the list of 1D coordinate variable names for the data set (additional 1D or scalar vars can also be added). This functionality will be the default in future releases of virtualizarr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75957e8b-2c74-49ac-9413-38cc54f86a49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will be assigned to 'loadable_variables' and needs to be modified per the specific \n",
    "# coord names of the data set:\n",
    "coord_vars = loadable_coord_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e23674f0-164c-4994-87ef-5a8d1bcf7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_opts = {\"storage_options\": fs.storage_options} # S3 filesystem creds from previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329db221-b2dc-41de-8e67-3405402e4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'time' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'lat' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'lon' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wvc_index' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'model_speed' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'model_dir' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'ice_prob' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'ice_age' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wind_speed' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wind_dir' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'bs_distance' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 13MB\n",
      "Dimensions:           (NUMROWS: 3264, NUMCELLS: 82)\n",
      "Coordinates:\n",
      "    lat               (NUMROWS, NUMCELLS) float32 1MB ...\n",
      "    lon               (NUMROWS, NUMCELLS) float32 1MB ...\n",
      "Dimensions without coordinates: NUMROWS, NUMCELLS\n",
      "Data variables:\n",
      "    time              (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    wvc_index         (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    model_speed       (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    model_dir         (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    ice_prob          (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    ice_age           (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    wvc_quality_flag  (NUMROWS, NUMCELLS) int32 1MB ManifestArray<shape=(3264...\n",
      "    wind_speed        (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    wind_dir          (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "    bs_distance       (NUMROWS, NUMCELLS) float32 1MB ManifestArray<shape=(32...\n",
      "Attributes: (29)\n",
      "CPU times: user 251 ms, sys: 106 ms, total: 356 ms\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create reference for the first data file:\n",
    "virtual_ds_example = open_virtual_dataset(\n",
    "    data_s3links[0], indexes={}, \n",
    "    reader_options=reader_opts, loadable_variables=coord_vars\n",
    "    )\n",
    "print(virtual_ds_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b850da-5df5-460e-81f3-0450a2904667",
   "metadata": {},
   "source": [
    "The reference can be saved to file and used to open the corresponding CCMP data file with Xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e53d64-6547-4193-a24e-6018bbad0476",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "virtual_ds_example.virtualize.to_kerchunk('virtual_ds_example.json', format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15707ccc-fcb3-4bf9-94cb-71b11d468054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data using the reference file, using a small wrapper function around xarray's open_dataset. \n",
    "# This will shorten code blocks in other sections. \n",
    "def opends_withref(ref, fs_data):\n",
    "    \"\"\"\n",
    "    \"ref\" is a reference file or object. \"fs_data\" is a filesystem with credentials to\n",
    "    access the actual data files. \n",
    "    \"\"\"\n",
    "    storage_opts = {\"fo\": ref, \"remote_protocol\": \"https\", \"remote_options\": fs_data.storage_options}\n",
    "    fs_ref = fsspec.filesystem('reference', **storage_opts)\n",
    "    m = fs_ref.get_mapper('')\n",
    "    data = xr.open_dataset(\n",
    "        m, engine=\"zarr\", chunks={},\n",
    "        backend_kwargs={\"consolidated\": False}\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b02a2f1c-2b53-4a0b-ba0e-2bc0a35fce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'bs_distance' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'ice_age' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'ice_prob' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'lat' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'lon' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'model_dir' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'model_speed' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'time' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 15MB\n",
      "Dimensions:           (NUMROWS: 3264, NUMCELLS: 82)\n",
      "Coordinates:\n",
      "    lat               (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    lon               (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "Dimensions without coordinates: NUMROWS, NUMCELLS\n",
      "Data variables:\n",
      "    bs_distance       (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    ice_age           (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    ice_prob          (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    model_dir         (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    model_speed       (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    time              (NUMROWS, NUMCELLS) datetime64[ns] 2MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    wind_dir          (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    wind_speed        (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    wvc_index         (NUMROWS, NUMCELLS) float32 1MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "    wvc_quality_flag  (NUMROWS, NUMCELLS) float64 2MB dask.array<chunksize=(3264, 82), meta=np.ndarray>\n",
      "Attributes: (29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wind_dir' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wind_speed' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/Users/gangl/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'wvc_index' has multiple fill values {np.float32(1e+30), np.float64(1e+30)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    }
   ],
   "source": [
    "data_example = opends_withref('virtual_ds_example.json', fs)\n",
    "print(data_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f09091b-8532-4e8f-a159-aa1a545683a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not find a Chunk Manager which recognises type <class 'virtualizarr.manifests.array.ManifestArray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvirtual_ds_example\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwind_speed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/plot/accessor.py:48\u001b[39m, in \u001b[36mDataArrayPlotAccessor.__call__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(dataarray_plot.plot, assigned=(\u001b[33m\"\u001b[39m\u001b[33m__doc__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m__annotations__\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataarray_plot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_da\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/plot/dataarray_plot.py:271\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(darray, row, col, col_wrap, ax, hue, subplot_kws, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m    221\u001b[39m     darray: DataArray,\n\u001b[32m    222\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     **kwargs: Any,\n\u001b[32m    230\u001b[39m ) -> Any:\n\u001b[32m    231\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m    Default plot of DataArray using :py:mod:`matplotlib:matplotlib.pyplot`.\u001b[39;00m\n\u001b[32m    233\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m \u001b[33;03m    xarray.DataArray.squeeze\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m     darray = \u001b[43mdarray\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdarray\u001b[49m\u001b[43m.\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     plot_dims = \u001b[38;5;28mset\u001b[39m(darray.dims)\n\u001b[32m    274\u001b[39m     plot_dims.discard(row)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/core/dataarray.py:1207\u001b[39m, in \u001b[36mDataArray.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1182\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[32m   1183\u001b[39m \u001b[33;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[32m   1184\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m \u001b[33;03mdask.compute\u001b[39;00m\n\u001b[32m   1205\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1206\u001b[39m new = \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/core/dataarray.py:1175\u001b[39m, in \u001b[36mDataArray.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs) -> Self:\n\u001b[32m   1156\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[32m   1157\u001b[39m \u001b[33;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[32m   1158\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1173\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m     new = \u001b[38;5;28mself\u001b[39m._from_temp_dataset(ds)\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28mself\u001b[39m._variable = new._variable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/core/dataset.py:538\u001b[39m, in \u001b[36mDataset.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m lazy_data = {\n\u001b[32m    535\u001b[39m     k: v._data \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.variables.items() \u001b[38;5;28;01mif\u001b[39;00m is_chunked_array(v._data)\n\u001b[32m    536\u001b[39m }\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lazy_data:\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     chunkmanager = \u001b[43mget_chunked_array_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlazy_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[32m    541\u001b[39m     evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n\u001b[32m    542\u001b[39m         *lazy_data.values(), **kwargs\n\u001b[32m    543\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cloud-optimize-notebook/lib/python3.12/site-packages/xarray/namedarray/parallelcompat.py:177\u001b[39m, in \u001b[36mget_chunked_array_type\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    171\u001b[39m selected = [\n\u001b[32m    172\u001b[39m     chunkmanager\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunkmanager \u001b[38;5;129;01min\u001b[39;00m chunkmanagers.values()\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunkmanager.is_chunked_array(chunked_arr)\n\u001b[32m    175\u001b[39m ]\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m selected:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    178\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find a Chunk Manager which recognises type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunked_arr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m     )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected) >= \u001b[32m2\u001b[39m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMultiple ChunkManagers recognise type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunked_arr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Could not find a Chunk Manager which recognises type <class 'virtualizarr.manifests.array.ManifestArray'>"
     ]
    }
   ],
   "source": [
    "virtual_ds_example.wind_speed.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051e7d34-594c-40dc-9219-6e281ccea777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 bytes\n"
     ]
    }
   ],
   "source": [
    "# Also useful to note, these reference objects don't take much memory:\n",
    "print(sys.getsizeof(virtual_ds_example), \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b4028-b31d-4c28-a78d-2fedebcf967c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 First year\n",
    "Reference information for each data file in the year is created individually, and then the combined reference file for the year can be created.\n",
    "\n",
    "For us, reference file creation for a single file takes about 0.7 seconds, so processing a year of files would take about 4.25 minuts. One can easly accomplish this with a for-loop:\n",
    "\n",
    "```\n",
    "virtual_ds_list = [\n",
    "    open_virtual_dataset(\n",
    "        p, indexes={},\n",
    "        reader_options={\"storage_options\": fs.storage_options},\n",
    "        loadable_variables=coord_vars\n",
    "        )\n",
    "    for p in data_s3links\n",
    "    ]\n",
    "```\n",
    "\n",
    "However, we speed things up using basic parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fc53c-2bf2-4b9d-9ab7-32a498011bd4",
   "metadata": {},
   "source": [
    "### 2.2.1 Method 1: parallelize using Dask local cluster\n",
    "If using an `m6i.4xlarge` AWS EC2 instance, there are 16 CPUs available and each should have enough memory to utilize all at once. If working on a different VM-type, change the `n_workers` in the call to `Client()` below as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6bae28c-0527-4ad8-8065-f6187fb46961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count = 12\n"
     ]
    }
   ],
   "source": [
    "# Check how many cpu's are on this VM:\n",
    "print(\"CPU count =\", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "081fdd0e-03c7-4746-9066-9c1342b99cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCluster(dad334ab, 'tcp://127.0.0.1:63197', workers=15, threads=15, memory=64.00 GiB)\n",
      "View any work being done on the cluster here http://127.0.0.1:8787/status\n",
      "Processing https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_20220103_V03.1_L4.nc\n",
      "Processing https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_20220101_V03.1_L4.nc\n",
      "Processing https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_20220102_V03.1_L4.nc\n"
     ]
    }
   ],
   "source": [
    "# Start up cluster and print some information about it:\n",
    "client = Client(n_workers=15, threads_per_worker=1)\n",
    "print(client.cluster)\n",
    "print(\"View any work being done on the cluster here\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be1f9c2e-98e5-4c61-9ba3-36965fb70bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unittest.mock import Mock\n",
    "\n",
    "# # Raise an exception class\n",
    "# open_virtual_dataset = Mock(side_effect=ValueError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0734361a-ee57-4769-aeb9-3e4be00e963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def open_vds_par(datalink, reader_options=None, loadable_variables=None):\n",
    "    for cnt  in range(1,5):\n",
    "        try:\n",
    "            if cnt == 1:\n",
    "                print(\"Processing \" +datalink)\n",
    "            else:\n",
    "                print(\"Retrying ({}) {} \".format(cnt, datalink))\n",
    "            return open_virtual_dataset(datalink, indexes={}, reader_options=reader_options,loadable_variables=loadable_variables )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.debug(e)\n",
    "            time.sleep(cnt**2)\n",
    "    raise Exception(\"Could not process file \" + datalink)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd722c4e-7b51-4807-b138-c9e1052ba465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.49 s, sys: 1.88 s, total: 5.37 s\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create individual references:\n",
    "#open_vds_par = delayed(open_virtual_dataset)\n",
    "tasks = [\n",
    "    open_vds_par(p, reader_options=reader_opts, loadable_variables=coord_vars) \n",
    "    for p in data_s3links # all files\n",
    "    ]\n",
    "virtual_ds_list = list(da.compute(*tasks)) # The xr.combine_nested() function below needs a list rather than a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e17f4-8a71-49ea-b18c-de13f2cebd2a",
   "metadata": {},
   "source": [
    "Using the individual references to create the combined reference is fast and does not requre parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45722c2f-a4c5-4249-a2d6-06761ff35052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.08 ms, sys: 2.18 ms, total: 6.26 ms\n",
      "Wall time: 5.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the combined reference\n",
    "virtual_ds_combined = xr.combine_nested(virtual_ds_list, concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1628e4a-0e46-45ec-bd9e-63922fd085f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in JSON or PARQUET format:\n",
    "fname_combined_json = 'ref_combined_1year.json'\n",
    "fname_combined_parq = 'ref_combined_1year.parq'\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_json, format='json')\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_parq, format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bfee1-087a-4290-83a3-609de26a519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file JSON:\n",
    "data_json = opends_withref(fname_combined_json, fs)\n",
    "print(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89451f0c-7453-4fee-843f-5faa196bcda1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file PARQUET:\n",
    "data_parq = opends_withref(fname_combined_parq, fs)\n",
    "print(data_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2292015-458f-440a-b5f2-f5f88521cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b66a25-1e90-40d1-a600-fea2f47193e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.3 Entire record\n",
    "\n",
    "Processing the entire record follows the exact same workflow as processing the first year Section 2.2 (either parallelization method). The only modification required is to change the one instance of \n",
    "```\n",
    "data_s3links[:365]\n",
    "```\n",
    "with \n",
    "```\n",
    "data_s3links[:]\n",
    "```\n",
    "when setting up the parallel computations (occurs once in each of Sections 2.2.1 and 2.2.2). Optionally, also change the saved file names e.g. from `ref_combined_1year.json` to `ref_combined_record.json`.\n",
    "\n",
    "For us, processing the entire record using a local cluster on an `m6i.4xlarge` EC2 instance, with 15 workers, took about 13 minutes. Using 20 `m6i.large` VM's on a distributed cluster with Coiled also took ~15 minutes and cost ~$0.40.\n",
    "\n",
    "Because the virtualizarr package is so efficient at combining many individual reference files together, and because the individual references have such small in-memory requirements, the workflows in Section 2.2 are assumed to scale to tens of thousands of files and TB's of data. However, this assumption will be tested as the techniques in the notebook are applied to progressively larger data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176122c-e85b-4bf1-aa70-804f50d396bb",
   "metadata": {},
   "source": [
    "For us, lazy loading the entire record took ~3 seconds. Compare that to an attempt at opening these same files with `Xarray` the \"traditional\" way with a call to `xr.open_mfdataset()`. On a smaller machine, the following line of code will either fail or take a long (possibly very long) amount of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ac19b-217b-472b-993a-0c560a228c30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You can try un-commenting and running this but your notebook will probably stall or crash:\n",
    "# fobjs = earthaccess.open(granule_info)\n",
    "# data = xr.open_mfdataset(fobjs[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4a16b-9a8a-421d-8cd1-dbeb984ec8fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will only work if you have the appropriate permissions! you either need an aws environment setup to allow access to 'bucket'\n",
    "# or have an IAM roles that has access to that bucket.\n",
    "## Store the files in S3\n",
    "# Upload the file\n",
    "s3_client = boto3.client('s3')\n",
    "try:\n",
    "#    response = s3_client.upload_file(fname_combined_parq, bucket, \"virtualcollection/{}/virtual.parq\".format(collection))\n",
    "    response = s3_client.upload_file(fname_combined_json, bucket, \"virtualcollection/{}/{}_virtual.json\".format(collection,collection))\n",
    "except ClientError as e:\n",
    "    logging.error(e)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de875c5-6610-4523-be10-83fb0432e7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
